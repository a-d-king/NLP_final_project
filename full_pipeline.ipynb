{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24884,"status":"ok","timestamp":1723473863833,"user":{"displayName":"Adam King","userId":"10853683565362636129"},"user_tz":300},"id":"4V0d-zME6bgK","outputId":"db3496f5-9573-4954-b681-271d57d46f55"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","# Mount your Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":95627,"status":"ok","timestamp":1723473972429,"user":{"displayName":"Adam King","userId":"10853683565362636129"},"user_tz":300},"id":"Z5FV1EzQ_Xok"},"outputs":[],"source":["%%capture\n","# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n","!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n","!pip install easyocr\n","!pip install openai"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":442,"status":"ok","timestamp":1723473987030,"user":{"displayName":"Adam King","userId":"10853683565362636129"},"user_tz":300},"id":"BWmcA23_4ivo"},"outputs":[],"source":["# Need to use Google Drive or replace image path to valid JPEG handwriting image\n","image_path = '/content/drive/My Drive/NLP Final Project Data/tarun_writing.jpeg'"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1723473988460,"user":{"displayName":"Adam King","userId":"10853683565362636129"},"user_tz":300},"id":"QyofYZ3u-xmC"},"outputs":[],"source":["# Need to use Google Drive or replace model path to valid model.pth file\n","model_path = '/content/drive/My Drive/NLP Final Project Data/IAM/best_model.pth'\n","\n","instructions = \"\"\"\n","\n","    You are a computer science expert and a skilled writer.\n","\n","    Craft detailed content about the given computer science subtopic for university-level lecture notes, targeting a total of about 500 words distributed over a few paragraphs.\n","\n","    Begin with an introductory paragraph that lays the foundation of the subtopic. Follow this with detailed paragraphs focusing on the critical aspects of the subtopic. Include applications only if they are essential for understanding the concept; otherwise, concentrate on explaining the concept itself and its nuances.\n","\n","    You can selectively, if necessary, use examples, tables in Markdown format to illustrate key points, ensuring that any code provided is concise and directly demonstrates the concept, otherwise you don't need to include it.\n","\n","    Please also avoid overly detailed explanations of complex algorithms unless they are central to the subtopic. Do not go overboard with technical details that may overwhelm students.\n","\n","    Let's try to avoid generating code unless its short and obvious, otherwise, focus on detailed explanations and if you use equations, please use inline HTML. Quick and simple inline equations can utilize HTML ampersand entity codes, such as:\n","\n","        h<sub>&theta;</sub>(x) = &theta;<sub>o</sub> x + &theta;<sub>1</sub>x\n","\n","    This method works in practically all Markdown and does not require any external libraries. Avoid using LaTeX. If you cannot express it in HTML, please avoid using equations. Unless the symbol is simple and can be represented in HTML and Markdown, avoid using those symbols.\n","\n","    Let's try to avoid generating code unless its short and obvious, otherwise, focus on detailed explanations and if you use equations, please use LaTeX format.\n","\n","    Maintain clear and concise language suitable for a 10th-grade reading level, using academic language where appropriate. Avoid overly technical jargon unless it is necessary for clarity.\n","\n","    Also avoid your conclusion paragraph in the end since the content should be detailed throughout.\n","\n","    The entire response must be in valid Markdown format and avoid the use of diagrams unless they can be effectively represented in Markdown. You must stay in our limit of 500 words.\n","\n","    LaTeX is impossible to use in Markdown, so please use HTML for equations. Do not use LaTeX.\n","\n","    Your input will always be a single computer science subtopic, and your output should not conclude with a summarizing paragraph but rather emphasize detailed explanation throughout.\n","\n","\n","    Now, please generate detailed content about the subtopic in Markdown:\n","\n","    \"\"\""]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6788,"status":"ok","timestamp":1723473998726,"user":{"displayName":"Adam King","userId":"10853683565362636129"},"user_tz":300},"id":"hiwQp2C43Z6L"},"outputs":[],"source":["import sys\n","import easyocr\n","import torch\n","from PIL import Image\n","from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n","\n","def load_ocr_model(model_path = None):\n","    \"\"\"\n","    Load the TrOCR model and processor for handwritten text recognition.\n","\n","    Args:\n","        model_path (str, optional): Path to a pre-trained model state dictionary. \n","                                    If provided, the model will be loaded from this path. \n","                                    Defaults to None, in which case the default TrOCR model \n","                                    'microsoft/trocr-base-handwritten' is loaded.\n","\n","    Returns:\n","        model (VisionEncoderDecoderModel): The loaded TrOCR model.\n","        processor (TrOCRProcessor): The processor used for preprocessing images \n","                                    and decoding model outputs.\n","    \"\"\"\n","    processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\n","    model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\n","    if model_path:\n","        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","    return model, processor\n","\n","def process_image(image_path, model_path = None):\n","    \"\"\"\n","    Process an image to perform text detection using EasyOCR and text recognition using a custom TrOCR model.\n","\n","    Args:\n","        image_path (str): Path to the image file to be processed.\n","        model_path (str, optional): Path to a pre-trained TrOCR model state dictionary. \n","                                    If provided, the model will be loaded from this path. \n","                                    Defaults to None, in which case the default TrOCR model \n","                                    'microsoft/trocr-base-handwritten' is used.\n","\n","    Returns:\n","        extracted_words (list): A list of words extracted from the image using the OCR model.\n","    \"\"\"\n","    # Initialize EasyOCR for text detection\n","    reader = easyocr.Reader(['en'])\n","\n","    # Load your custom TrOCR model\n","    model, processor = load_ocr_model(model_path)\n","\n","    # Perform text detection and recognition using EasyOCR\n","    easyocr_results = reader.readtext(image_path)\n","\n","    # Open the full image\n","    full_image = Image.open(image_path)\n","\n","    extracted_words = []\n","\n","    print(\"Processing results:\")\n","    for idx, (bbox, easyocr_text, prob) in enumerate(easyocr_results, 1):\n","        print(f\"\\nProcessing text region {idx}:\")\n","        print(f\"EasyOCR Text: {easyocr_text}\")\n","        print(f\"Probability: {prob}\")\n","\n","        # Calculate coordinates for cropping\n","        x_min = min(point[0] for point in bbox)\n","        y_min = min(point[1] for point in bbox)\n","        x_max = max(point[0] for point in bbox)\n","        y_max = max(point[1] for point in bbox)\n","\n","        # Ensure coordinates are integers\n","        x_min, y_min, x_max, y_max = map(int, [x_min, y_min, x_max, y_max])\n","\n","        # Crop the image based on the bounding box\n","        cropped_image = full_image.crop((x_min, y_min, x_max, y_max))\n","\n","        # Preprocess the cropped image\n","        pixel_values = processor(cropped_image, return_tensors=\"pt\").pixel_values\n","\n","        # Generate text using your model\n","        generated_ids = model.generate(pixel_values)\n","\n","        # Decode the generated ids\n","        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","        print(f\"Your OCR Model Generated text: {generated_text}\")\n","\n","        # Add words to the list\n","        extracted_words.extend(generated_text.split())\n","\n","    print(\"\\nExtracted Words:\")\n","    print(extracted_words)\n","\n","    return extracted_words"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220,"referenced_widgets":["2f664363d5da46d18b545a1996100d04","627a061683574cccb4e01c0349fc6fc5","347757a30fd142188fc694ed8af12a1f","f884fc8e317b47cdba8ea6790a62947c","83bcef3a13c343a4bad451e43538916f","95fe9cb0377f4b8cbdf252247a68afeb","ebc3aa134baa41d0ab24ac83f76aca04","74da667eb4ae491195e037264a923536","4e11a2e08f404d78a03ddd8acfee1ce7","a65c25f1266a429d9e2ddabf33f8871f","44e73ac499384af884668efa4fd1fdbc","b055ba4d24fd4a79a2b7f4e73d8cc526","147f5fba6c12446a945db8b137842987","d5052ee7f8164fe29541427e90bdb418","a49fec14203443ab8608a59c1e7385f5","b3dff21e28b440eea8221a3ade16ccc2","9d716618c73542339c022d562c582b0d","064d3876c6b34e3ab80bd17f890b0c28","40cfe9071b7e4e7989510b911010b45b","094bf3a0ae604539b32ce84103f54db7","c5b0804bb8f54c38bc170d93a373d4d8","7f467db40c25438bab908374cbaa482f"]},"executionInfo":{"elapsed":53468,"status":"ok","timestamp":1723474057179,"user":{"displayName":"Adam King","userId":"10853683565362636129"},"user_tz":300},"id":"zN0r1CRh-9wz","outputId":"1e9ff877-8868-4c0e-a004-9c415dc222c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n","   \\\\   /|    GPU: NVIDIA L4. Max memory: 22.168 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f664363d5da46d18b545a1996100d04","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b055ba4d24fd4a79a2b7f4e73d8cc526","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"]}],"source":["from unsloth import FastLanguageModel\n","\n","# Reuse the same parameters from training\n","max_seq_length = 2048\n","dtype = None  # None for auto detection\n","load_in_4bit = True\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"/content/drive/My Drive/NLP Final Project Data/lora_model\",  # The directory where your model was saved\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n",")\n","\n","FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n","\n","# Define the prompt template\n","prompt_template = \"\"\"{instructions}\n","\n","###INPUT (Notes):\n","{input}\n","\n","###OUTPUT (Expected Generations):\n","{output}\"\"\"\n","\n","# Function to generate text\n","def generate_text(input_text, max_new_tokens=1000):\n","    \"\"\"\n","    Generate text based on the input text using our fine-tuned Llama model.\n","\n","    Args:\n","        input_text (str): The input text or notes that will be expanded upon.\n","        max_new_tokens (int, optional): The maximum number of new tokens to generate. \n","                                        Defaults to 1000.\n","\n","    Returns:\n","        str: The generated text based on the input.\n","    \"\"\"\n","    prompt = prompt_template.format(\n","        instructions=instructions,  # Use the instructions from your training\n","        input=input_text,\n","        output=\"\"  # Leave this blank for generation\n","    )\n","\n","    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            **inputs,\n","            max_new_tokens=1000,\n","            use_cache=True,\n","            temperature=0.7,  # Adjust as needed\n","            top_p=0.9,  # Adjust as needed\n","        )\n","\n","    return tokenizer.decode(outputs[0], skip_special_tokens=True)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":1522,"status":"ok","timestamp":1723474106331,"user":{"displayName":"Adam King","userId":"10853683565362636129"},"user_tz":300},"id":"5g7GLsKpC635"},"outputs":[],"source":["import openai\n","\n","def grammar_check(ocr_text):\n","  \"\"\"\n","  Corrects the spelling and grammar of text generated by an OCR model using OpenAI's GPT model.\n","\n","  Args:\n","    ocr_text (str): The text generated by the OCR model that needs to be corrected.\n","\n","  Returns:\n","    str: The corrected text with improved grammar and spelling, while maintaining the original meaning and structure.\n","  \"\"\"\n","  # Initialize the OpenAI client\n","  client = openai.OpenAI()\n","\n","  OCR_CORRECTION_PROMPT = \"\"\"\n","  Correct the spelling and grammar in the following text, which was generated by an OCR model. Maintain the original meaning and structure of the sentence as much as possible. Only make changes necessary for clarity and correctness. Do not add new information or significantly alter the sentence structure. Here's the text to correct:\n","\n","  \"{text}\"\n","\n","  Provide the corrected version of the text.\n","  \"\"\"\n","\n","  prompt = OCR_CORRECTION_PROMPT.format(text=ocr_text)\n","\n","  response = client.chat.completions.create(\n","      model=\"gpt-4o-mini\",  # Use the appropriate model\n","      messages=[\n","          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","          {\"role\": \"user\", \"content\": prompt}\n","      ],\n","      max_tokens=200  # Adjust as needed for longer content\n","  )\n","\n","  corrected_text = response.choices[0].message.content.strip()\n","\n","  return corrected_text\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":932,"status":"ok","timestamp":1723474110624,"user":{"displayName":"Adam King","userId":"10853683565362636129"},"user_tz":300},"id":"aC0xEnLy21l4"},"outputs":[],"source":["def load_image(image_path):\n","    \"\"\"\n","    Load an image from the specified file path.\n","\n","    Args:\n","        image_path (str): The path to the image file to be loaded.\n","\n","    Returns:\n","        PIL.Image.Image: The loaded image as a PIL Image object.\n","    \"\"\"\n","    try:\n","        image = Image.open(image_path)\n","        return image\n","    except Exception as e:\n","        print(f\"Error loading image: {e}\")\n","        sys.exit(1)\n","\n","def pipeline(image_path):\n","    \"\"\"\n","    Process an image through a text recognition and expansion pipeline.\n","\n","    This function loads an image, processes it using an OCR model, performs grammar validation, and generates an expanded text. The final output is saved to a markdown file.\n","\n","    Args:\n","        image_path (str): The path to the image file to be processed.\n","\n","    Returns:\n","        str: The final generated text after processing and expansion.\n","\n","    Workflow:\n","        1. Load the image using `load_image`.\n","        2. Process the image with the vision model to extract words.\n","        3. Join the extracted words into a full OCR output text.\n","        4. Validate the grammar of the OCR text using `grammar_check`.\n","        5. Generate expanded text using a language model.\n","        6. Save the final expanded text to a markdown file.\n","\n","    Example:\n","        generated_text = pipeline(\"path/to/image.jpg\")\n","    \"\"\"\n","    # Load the image\n","    image = load_image(image_path)\n","\n","    # Process the image with the vision mode\n","    processed_image_word_list = process_image(image)\n","\n","    # Join the list of output values from the vision model pipeline\n","    full_ocr_output_text = \" \".join(processed_image_word_list)\n","\n","    print(f\"Pre-validated OCR text: {full_ocr_output_text}\")\n","\n","    # Grammar check\n","    grammar_validated_text = grammar_check(full_ocr_output_text)\n","\n","    print(f\"Post-validated OCR text: {grammar_validated_text}\")\n","\n","    # Into Llama model\n","    generated_text = generate_text(grammar_validated_text)\n","\n","    output_file_path = \"/content/drive/My Drive/NLP Final Project Data/output/expansion_text.md\"\n","\n","    # Save the combined text to the markdown file\n","    with open(output_file_path, \"w\") as file:\n","      file.write(generated_text)\n","\n","    return generated_text"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60707,"status":"ok","timestamp":1723474175958,"user":{"displayName":"Adam King","userId":"10853683565362636129"},"user_tz":300},"id":"ypYx6Aux438z","outputId":"aa59dc05-60c6-4a40-9cbb-dd263cdad170"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","    You are a computer science expert and a skilled writer.\n","\n","    Craft detailed content about the given computer science subtopic for university-level lecture notes, targeting a total of about 500 words distributed over a few paragraphs.\n","\n","    Begin with an introductory paragraph that lays the foundation of the subtopic. Follow this with detailed paragraphs focusing on the critical aspects of the subtopic. Include applications only if they are essential for understanding the concept; otherwise, concentrate on explaining the concept itself and its nuances.\n","\n","    You can selectively, if necessary, use examples, tables in Markdown format to illustrate key points, ensuring that any code provided is concise and directly demonstrates the concept, otherwise you don't need to include it.\n","\n","    Please also avoid overly detailed explanations of complex algorithms unless they are central to the subtopic. Do not go overboard with technical details that may overwhelm students.\n","\n","    Let's try to avoid generating code unless its short and obvious, otherwise, focus on detailed explanations and if you use equations, please use inline HTML. Quick and simple inline equations can utilize HTML ampersand entity codes, such as:\n","\n","        h<sub>&theta;</sub>(x) = &theta;<sub>o</sub> x + &theta;<sub>1</sub>x\n","\n","    This method works in practically all Markdown and does not require any external libraries. Avoid using LaTeX. If you cannot express it in HTML, please avoid using equations. Unless the symbol is simple and can be represented in HTML and Markdown, avoid using those symbols.\n","\n","    Let's try to avoid generating code unless its short and obvious, otherwise, focus on detailed explanations and if you use equations, please use LaTeX format.\n","\n","    Maintain clear and concise language suitable for a 10th-grade reading level, using academic language where appropriate. Avoid overly technical jargon unless it is necessary for clarity.\n","\n","    Also avoid your conclusion paragraph in the end since the content should be detailed throughout.\n","\n","    The entire response must be in valid Markdown format and avoid the use of diagrams unless they can be effectively represented in Markdown. You must stay in our limit of 500 words.\n","\n","    LaTeX is impossible to use in Markdown, so please use HTML for equations. Do not use LaTeX.\n","\n","    Your input will always be a single computer science subtopic, and your output should not conclude with a summarizing paragraph but rather emphasize detailed explanation throughout.\n","\n","\n","    Now, please generate detailed content about the subtopic in Markdown:\n","\n","    \n","\n","###INPUT (Notes):\n","A B-Tree is a self-balancing tree data structure that maintains sorted data and allows for efficient insertion, deletion, and search operations, making it ideal for database indexing\n","\n","###OUTPUT (Expected Generations):\n","# B-Trees: A Self-Balancing Tree Structure for Efficient Data Storage\n","\n","B-Trees are a fundamental data structure used primarily in databases for the efficient management of large datasets. They are particularly effective in scenarios where data must be stored and retrieved in a sorted manner, such as in database indexing. B-Trees maintain a balanced structure, which ensures that operations like insertion, deletion, and search are performed in logarithmic time. This property is crucial for applications that require frequent updates and queries on large volumes of data.\n","\n","## Structure of a B-Tree\n","\n","A B-Tree is a balanced tree data structure that maintains sorted data and allows for multiple keys per node. Each node in a B-Tree contains a fixed number of keys, referred to as the \"order\" of the B-Tree. The keys in a node are arranged in sorted order, and each node can have multiple children. The children of a node represent the next levels of the tree, and each child points to the root of a subtree.\n","\n","For example, in a B-Tree of order 3, a node may contain 2 keys and 2 children. The keys are sorted, so if a node has keys [10, 20], it would have two children pointing to nodes containing values less than 10 and greater than 20, respectively. This structure ensures that the tree remains balanced, which means that no leaf node is more than a certain distance (defined by the order) from the root.\n","\n","## Properties of B-Trees\n","\n","B-Trees exhibit several important properties:\n","\n","1. **Balanced Structure**: A B-Tree of order k ensures that each node has at most k-1 keys and at least ⌈k/2⌉ children. This property guarantees that the height of the tree is kept to a minimum, leading to efficient search times.\n","\n","2. **Sorted Keys**: The keys in each node are stored in a sorted order. This allows for efficient binary search operations within a node, making search and insertion operations quick.\n","\n","3. **Multi-Child Nodes**: B-Trees allow multiple children per node, which is essential for handling large datasets. This reduces the number of levels in the tree, improving performance for operations that traverse multiple nodes.\n","\n","4. **Leaf Nodes**: All leaf nodes are at the same depth, which means that all leaves are equidistant from the root. This ensures that the tree remains balanced and efficient for operations that require traversing the entire tree.\n","\n","## Operations on B-Trees\n","\n","### Insertion\n","\n","When inserting a key into a B-Tree, the process begins at the root node and follows the binary search algorithm to determine the correct position for the new key. If the node where the key is inserted exceeds the maximum number of keys (k-1), the node splits, and its keys are distributed between two new nodes. The parent node may also need to be updated to accommodate the new nodes.\n","\n","### Deletion\n","\n","Deletion in a B-Tree is similar to insertion. The process involves removing a key from a node and possibly merging with adjacent nodes if the node falls below the minimum number of keys required (⌈k/2⌉). If a node is left with too few keys, it can be merged with its siblings to maintain the tree's structure and properties.\n","\n","### Search\n","\n","Searching for a key in a B-Tree follows the same principles as insertion. The search starts at the root, traversing down the tree based on key comparisons. The process continues until the desired key is found or a leaf node is reached where the key does not exist.\n","\n","### Applications\n","\n","B-Trees are widely used in database management systems for indexing and sorting data. By utilizing B-Trees, databases can efficiently handle queries and updates, improving overall performance. Additionally, B-Trees are particularly effective for large datasets due to their ability to handle multiple keys per node, thereby reducing the depth of the tree and enhancing read and write operations.\n","\n","In summary, B-Trees are a crucial data structure that facilitates efficient data storage and retrieval, making them a staple in the field of computer science and database management. Their self-balancing properties and multi-key capabilities ensure that they remain a robust choice for maintaining sorted data, particularly in environments where data integrity and performance are paramount.\n"]}],"source":["# Call model pipeline function with the relative path\n","print(pipeline(image_path))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"064d3876c6b34e3ab80bd17f890b0c28":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"094bf3a0ae604539b32ce84103f54db7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"147f5fba6c12446a945db8b137842987":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d716618c73542339c022d562c582b0d","placeholder":"​","style":"IPY_MODEL_064d3876c6b34e3ab80bd17f890b0c28","value":"generation_config.json: 100%"}},"2f664363d5da46d18b545a1996100d04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_627a061683574cccb4e01c0349fc6fc5","IPY_MODEL_347757a30fd142188fc694ed8af12a1f","IPY_MODEL_f884fc8e317b47cdba8ea6790a62947c"],"layout":"IPY_MODEL_83bcef3a13c343a4bad451e43538916f"}},"347757a30fd142188fc694ed8af12a1f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_74da667eb4ae491195e037264a923536","max":5702746390,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e11a2e08f404d78a03ddd8acfee1ce7","value":5702746390}},"40cfe9071b7e4e7989510b911010b45b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44e73ac499384af884668efa4fd1fdbc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e11a2e08f404d78a03ddd8acfee1ce7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"627a061683574cccb4e01c0349fc6fc5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95fe9cb0377f4b8cbdf252247a68afeb","placeholder":"​","style":"IPY_MODEL_ebc3aa134baa41d0ab24ac83f76aca04","value":"model.safetensors: 100%"}},"74da667eb4ae491195e037264a923536":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f467db40c25438bab908374cbaa482f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83bcef3a13c343a4bad451e43538916f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95fe9cb0377f4b8cbdf252247a68afeb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d716618c73542339c022d562c582b0d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a49fec14203443ab8608a59c1e7385f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5b0804bb8f54c38bc170d93a373d4d8","placeholder":"​","style":"IPY_MODEL_7f467db40c25438bab908374cbaa482f","value":" 230/230 [00:00&lt;00:00, 20.6kB/s]"}},"a65c25f1266a429d9e2ddabf33f8871f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b055ba4d24fd4a79a2b7f4e73d8cc526":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_147f5fba6c12446a945db8b137842987","IPY_MODEL_d5052ee7f8164fe29541427e90bdb418","IPY_MODEL_a49fec14203443ab8608a59c1e7385f5"],"layout":"IPY_MODEL_b3dff21e28b440eea8221a3ade16ccc2"}},"b3dff21e28b440eea8221a3ade16ccc2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5b0804bb8f54c38bc170d93a373d4d8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5052ee7f8164fe29541427e90bdb418":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_40cfe9071b7e4e7989510b911010b45b","max":230,"min":0,"orientation":"horizontal","style":"IPY_MODEL_094bf3a0ae604539b32ce84103f54db7","value":230}},"ebc3aa134baa41d0ab24ac83f76aca04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f884fc8e317b47cdba8ea6790a62947c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a65c25f1266a429d9e2ddabf33f8871f","placeholder":"​","style":"IPY_MODEL_44e73ac499384af884668efa4fd1fdbc","value":" 5.70G/5.70G [00:19&lt;00:00, 355MB/s]"}}}}},"nbformat":4,"nbformat_minor":0}
